{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of automatic differentiation for the purpose of machine learning.\n",
    "\n",
    "implementing proximal gradient descent for minimization of a squared error loss function\n",
    "\n",
    "source: https://discourse.julialang.org/t/types-and-gradients-including-forward-gradient/946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package ForwardDiff not found in current path:\n- Run `Pkg.add(\"ForwardDiff\")` to install the ForwardDiff package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package ForwardDiff not found in current path:\n- Run `Pkg.add(\"ForwardDiff\")` to install the ForwardDiff package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at ./loading.jl:817",
      " [2] top-level scope at In[7]:1"
     ]
    }
   ],
   "source": [
    "using ForwardDiff: gradient, derivative\n",
    "using LinearAlgebra, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "linear_regression(w, b, x) = w*x .+ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_squared_error (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function\n",
    "mean_squared_error(ŷ, y) = sum(abs2, ŷ .- y) / size(y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss∇w (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get gradient w.r.t to `w`\n",
    "loss∇w(model, loss, w, b, x, y) = gradient(w -> loss(model(w, b, x), y), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lossdb (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get derivative w.r.t to `b`\n",
    "#\n",
    "# (`derivative` is used instead of `gradient` because `b` is a scalar instead of an array)\n",
    "lossdb(model, loss, w, b, x, y) = derivative(b -> loss(model(w, b, x), y), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proximal_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimization algorithm\n",
    "function proximal_gradient_descent(model, loss, w, b, x, y; lr=.1)\n",
    "    w -= lmul!(lr, loss∇w(model, loss, w, b, x, y))\n",
    "    b -= lr * lossdb(model, loss, w, b, x, y)\n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 3 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main(T=Array, n = 100000)\n",
    "    Random.seed!(0)\n",
    "\n",
    "    p = 25\n",
    "    x = randn(n,p)'\n",
    "    y = sum(x[1:5,:]; dims=1) .+ randn(n)'*0.1\n",
    "\n",
    "    w = 0.0001*randn(1,p)\n",
    "    b = 0.0\n",
    "\n",
    "    x = T(x)\n",
    "    y = T(y)\n",
    "    w = T(w)\n",
    "\n",
    "    model = linear_regression\n",
    "    loss = mean_squared_error\n",
    "    @time for i=1:p\n",
    "        w, b = proximal_gradient_descent(model, loss, w, b, x, y)\n",
    "        println(loss(model(w,b,x),y))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: Random not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Random not defined",
      "",
      "Stacktrace:",
      " [1] main(::Type, ::Int64) at ./In[6]:2 (repeats 2 times)",
      " [2] top-level scope at In[9]:1"
     ]
    }
   ],
   "source": [
    "main(Array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- play around with this code and define your own model or loss function\n",
    "- is this implementation generic enough that it just *works* with different array types\n",
    "- try using either `CuArrays` or `DArrays`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
